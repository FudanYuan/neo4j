{"title": "Neural Variational Inference and Learning in Undirected Graphical Models.", "fields": ["partition function", "generative grammar", "inference", "parametrization", "graphical model"], "abstract": "Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.", "citation": "Citations (1)", "year": "2017", "departments": ["Stanford University", "Stanford University"], "conf": "nips", "authors": ["Volodymyr Kuleshov.....http://dblp.org/pers/hd/k/Kuleshov:Volodymyr", "Stefano Ermon.....http://dblp.org/pers/hd/e/Ermon:Stefano"], "pages": 10}