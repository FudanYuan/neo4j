{"title": "Fast Probabilistic Optimization from Noisy Gradients.", "fields": ["neighbourhood components analysis", "stochastic gradient descent", "backpropagation", "nonlinear conjugate gradient method", "hessian matrix"], "abstract": "Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.", "citation": "Citations (13)", "departments": ["Max Planck Society"], "authors": ["Philipp Hennig.....http://dblp.org/pers/hd/h/Hennig:Philipp"], "conf": "icml", "year": "2013", "pages": 9}