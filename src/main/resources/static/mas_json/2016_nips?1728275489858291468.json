{"title": "Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning.", "fields": ["exploit", "sample complexity", "maximization", "generative model", "markov decision process"], "abstract": "You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.", "citation": "Not cited", "departments": ["French Institute for Research in Computer Science and Automation", "French Institute for Research in Computer Science and Automation", "Google"], "authors": ["Jean-Bastien Grill.....http://dblp.org/pers/hd/g/Grill:Jean=Bastien", "Michal Valko.....http://dblp.org/pers/hd/v/Valko:Michal", "R\u00e9mi Munos.....http://dblp.org/pers/hd/m/Munos:R=eacute=mi"], "conf": "nips", "year": "2016", "pages": 9}