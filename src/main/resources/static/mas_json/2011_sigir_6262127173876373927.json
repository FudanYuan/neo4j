{"title": "Regularized latent semantic indexing.", "fields": ["scaling", "probabilistic latent semantic analysis", "regularization", "topic model", "ranking"], "abstract": "Topic modeling can boost the performance of information retrieval, but its real-world application is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps such as vastly reducing input vocabulary. We introduce Regularized Latent Semantic Indexing (RLSI), a new method which is designed for parallelization. It is as effective as existing topic models, and scales to larger datasets without reducing input vocabulary. RLSI formalizes topic modeling as a problem of minimizing a quadratic loss function regularized by  l \u2082 and/or  l \u2081 norm. This formulation allows the learning process to be decomposed into multiple sub-optimization problems which can be optimized in parallel, for example via MapReduce. We particularly propose adopting  l \u2082 norm on topics and  l \u2081 norm on document representations, to create a model with compact and readable topics and useful for retrieval. Relevance ranking experiments on three TREC datasets show that RLSI performs better than LSI, PLSI, and LDA, and the improvements are sometimes statistically significant. Experiments on a web dataset, containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous studies.", "citation": "Citations (83)", "departments": ["Peking University", "Microsoft", "Microsoft", "Microsoft"], "authors": ["Quan Wang.....http://dblp.org/pers/hd/w/Wang:Quan", "Jun Xu.....http://dblp.org/pers/hd/x/Xu_0001:Jun", "Hang Li.....http://dblp.org/pers/hd/l/Li_0001:Hang", "Nick Craswell.....http://dblp.org/pers/hd/c/Craswell:Nick"], "conf": "sigir", "year": "2011", "pages": 10}