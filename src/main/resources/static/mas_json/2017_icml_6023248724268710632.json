{"title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions.", "fields": ["perplexity", "generative grammar", "language model", "dilation", "encoder"], "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.", "citation": "Citations (23)", "year": "2017", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University"], "conf": "icml", "authors": ["Zichao Yang.....http://dblp.org/pers/hd/y/Yang:Zichao", "Zhiting Hu.....http://dblp.org/pers/hd/h/Hu:Zhiting", "Ruslan Salakhutdinov.....http://dblp.org/pers/hd/s/Salakhutdinov:Ruslan", "Taylor Berg-Kirkpatrick.....http://dblp.org/pers/hd/b/Berg=Kirkpatrick:Taylor"], "pages": 10}