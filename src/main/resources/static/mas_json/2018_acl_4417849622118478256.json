{"title": "A Graph-to-Sequence Model for AMR-to-Text Generation.", "fields": ["graph", "machine learning", "artificial intelligence", "semantics", "encoding"], "abstract": "The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.", "citation": "Citations (3)", "year": "2018", "departments": ["Chinese Academy of Sciences", "Singapore University of Technology and Design", "University of Rochester"], "conf": "acl", "authors": ["Daniel Gildea.....http://dblp.org/pers/hd/g/Gildea:Daniel", "Zhiguo Wang.....http://dblp.org/pers/hd/w/Wang:Zhiguo", "Yue Zhang.....http://dblp.org/pers/hd/z/Zhang:Yue", "Linfeng Song.....http://dblp.org/pers/hd/s/Song:Linfeng"], "pages": 11}