{"title": "Word Embedding Causes Topic Shifting; Exploit Global Context!", "fields": ["word embedding", "exploit", "word2vec", "artificial intelligence", "natural language processing"], "abstract": "Exploitation of term relatedness provided by word embedding has gained considerable attention in recent IR literature. However, an emerging question is whether this sort of relatedness fits to the needs of IR with respect to retrieval effectiveness. While we observe a high potential of word embedding as a resource for related terms, the incidence of several cases of topic shifting deteriorates the final performance of the applied retrieval models. To address this issue, we revisit the use of global context (i.e. the term co-occurrence in documents) to measure the term relatedness. We hypothesize that in order to avoid topic shifting among the terms with high word embedding similarity, they should often share similar global contexts as well. We therefore study the effectiveness of post filtering of related terms by various global context relatedness measures. Experimental results show significant improvements in two out of three test collections, and support our initial hypothesis regarding the importance of considering global context in retrieval.", "citation": "Citations (5)", "departments": ["Vienna University of Technology", "Vienna University of Technology", "Vienna University of Technology", "University of Massachusetts Amherst"], "authors": ["Navid Rekabsaz.....http://dblp.org/pers/hd/r/Rekabsaz:Navid", "Mihai Lupu.....http://dblp.org/pers/hd/l/Lupu:Mihai", "Allan Hanbury.....http://dblp.org/pers/hd/h/Hanbury:Allan", "Hamed Zamani.....http://dblp.org/pers/hd/z/Zamani:Hamed"], "conf": "sigir", "year": "2017", "pages": 4}