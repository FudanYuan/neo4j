{"title": "Shampoo: Preconditioned Stochastic Tensor Optimization.", "fields": ["deep learning", "inequality", "tensor", "trace", "stochastic optimization"], "abstract": "Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.", "citation": "Citations (1)", "departments": ["Google", "Google"], "authors": ["Vineet Gupta.....http://dblp.org/pers/hd/g/Gupta_0001:Vineet", "Tomer Koren.....http://dblp.org/pers/hd/k/Koren:Tomer", "Yoram Singer.....http://dblp.org/pers/hd/s/Singer:Yoram"], "conf": "icml", "year": "2018", "pages": 9}