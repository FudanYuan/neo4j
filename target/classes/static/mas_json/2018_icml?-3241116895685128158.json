{"title": "Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing-and Back.", "fields": ["deep learning", "machine learning", "computer science", "artificial intelligence", "multi task learning"], "abstract": "Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.", "citation": "Citations (1)", "departments": ["University of Texas at Austin", "University of Texas at Austin"], "authors": ["Elliot Meyerson.....http://dblp.org/pers/hd/m/Meyerson:Elliot", "Risto Miikkulainen.....http://dblp.org/pers/hd/m/Miikkulainen:Risto"], "conf": "icml", "year": "2018", "pages": 10}